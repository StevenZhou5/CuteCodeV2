{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 23\n",
      "(None, 2, 23, 32)\n",
      "(None, 2, 23, 32)\n",
      "(None, 2, 23, 64)\n",
      "(2, 23, 23)\n",
      "<dtype: 'float32'> <dtype: 'float32'> <dtype: 'float32'>\n",
      "(None, 23, 2, 64)\n",
      "(None, 23, 128)\n"
     ]
    }
   ],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-Head Convolutional Self Attention Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dk, dv, num_heads, filter_size):\n",
    "        super().__init__()\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.conv_q = tf.keras.layers.Conv1D(dk * num_heads, # \n",
    "                                             filter_size, # \n",
    "                                             padding='causal',\n",
    "                                             dilation_rate=1)\n",
    "        self.conv_k = tf.keras.layers.Conv1D(dk * num_heads, filter_size, padding='causal',dilation_rate=1)\n",
    "        self.dense_v = tf.keras.layers.Dense(dv * num_heads)\n",
    "        self.dense1 = tf.keras.layers.Dense(dv, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(dv)\n",
    "        \n",
    "    def split_heads(self, x, batch_size, dim):\n",
    "        x = tf.expand_dims(x,axis=1)\n",
    "        x = tf.concat(tf.split(x,self.num_heads,axis=3),axis=1)\n",
    "#         print(\"x的shape:\",x.shape) # (1, 6, 23, 64)\n",
    "        return x\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size, time_steps, _ = inputs.shape\n",
    "        print(batch_size,time_steps)\n",
    "        \n",
    "        q = self.conv_q(inputs)\n",
    "        k = self.conv_k(inputs)\n",
    "        v = self.dense_v(inputs)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size, self.dk)\n",
    "        k = self.split_heads(k, batch_size, self.dk)\n",
    "        v = self.split_heads(v, batch_size, self.dv)\n",
    "        print(q.shape)\n",
    "        print(k.shape)\n",
    "        print(v.shape)\n",
    "        \n",
    "        # \n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((self.num_heads, time_steps, time_steps)), -1, 0)\n",
    "        print(mask.shape) # mask的第一个维度不需要设置，会自动的扩维\n",
    "        \n",
    "        dk = tf.cast(self.dk, tf.float32)\n",
    "        print(dk.dtype,q.dtype,k.dtype)\n",
    "        \n",
    "        score = tf.nn.softmax(tf.matmul(q, k, transpose_b=True)/tf.math.sqrt(dk) + mask * -1e9)\n",
    "        \n",
    "        outputs = tf.matmul(score, v)\n",
    "        \n",
    "        outputs = tf.transpose(outputs, perm=[0, 2, 1, 3])\n",
    "        print(outputs.shape)\n",
    "#         outputs = tf.reshape(outputs, (batch_size, time_steps, -1))\n",
    "        outputs = tf.squeeze(tf.concat(tf.split(outputs,self.num_heads,axis=2),axis=-1),axis=-2)\n",
    "        print(outputs.shape)\n",
    "        \n",
    "        outputs = self.dense1(outputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        \n",
    "        return outputs\n",
    "input_layer = keras.layers.Input(shape=(23,1))\n",
    "q_k_dim = 32\n",
    "v_dim = 64\n",
    "num_heads = 2\n",
    "kernel_size = 2 # 这个应该是kernel_size\n",
    "attention_layer = AttentionLayer(q_k_dim,v_dim,num_heads,kernel_size)(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 23\n",
      "(1, 23, 384)\n",
      "(1, 23, 384)\n",
      "(1, 23, 384)\n",
      "(1, 6, 23, 64)\n",
      "(1, 6, 23, 64)\n",
      "(1, 6, 23, 64)\n",
      "(6, 23, 23)\n",
      "<dtype: 'float32'> <dtype: 'float32'> <dtype: 'float32'>\n",
      "(1, 23, 6, 64)\n",
      "(1, 23, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 23, 64), dtype=float32, numpy=\n",
       "array([[[ 0.11008102,  0.07459726, -0.08492145, ..., -0.11901394,\n",
       "          0.08982512,  0.06120256],\n",
       "        [ 0.23853333,  0.1825556 , -0.19826752, ..., -0.2719772 ,\n",
       "          0.22004394,  0.15924905],\n",
       "        [ 0.25555617,  0.19842339, -0.22079235, ..., -0.30405885,\n",
       "          0.2452208 ,  0.17303891],\n",
       "        ...,\n",
       "        [ 0.276011  ,  0.14434591, -0.35215068, ..., -0.589652  ,\n",
       "          0.42293638,  0.66774774],\n",
       "        [ 0.4504622 ,  0.35425574, -0.4111826 , ..., -0.57004464,\n",
       "          0.46835276,  0.5105071 ],\n",
       "        [ 0.43305025,  0.34583712, -0.41320977, ..., -0.5793208 ,\n",
       "          0.474707  ,  0.5581056 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selfAttention处理序列数据\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-Head Convolutional Self Attention Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dk, dv, num_heads, filter_size):\n",
    "        super().__init__()\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.conv_q = tf.keras.layers.Conv1D(dk * num_heads, # \n",
    "                                             filter_size, # \n",
    "                                             padding='causal',\n",
    "                                             dilation_rate=1)\n",
    "        self.conv_k = tf.keras.layers.Conv1D(dk * num_heads, filter_size, padding='causal',dilation_rate=1)\n",
    "        self.dense_v = tf.keras.layers.Dense(dv * num_heads)\n",
    "        self.dense1 = tf.keras.layers.Dense(dv, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(dv)\n",
    "        \n",
    "    def split_heads(self, x, batch_size, dim):\n",
    "#         x = tf.reshape(x, (batch_size, -1, self.num_heads, dim))\n",
    "#         x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "#         print(\"x的shape:\",x.shape) # (1, 6, 23, 64)\n",
    "#         return x\n",
    "        \n",
    "        # 用这种方式才能避免eager模型下shape处理\n",
    "        x = tf.expand_dims(x,axis=1)\n",
    "        x = tf.concat(tf.split(x,self.num_heads,axis=3),axis=1)\n",
    "#         print(\"x的shape:\",x.shape) # (1, 6, 23, 64)\n",
    "        return x\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size, time_steps, _ = inputs.shape\n",
    "        print(batch_size,time_steps)\n",
    "        \n",
    "        q = self.conv_q(inputs)\n",
    "        k = self.conv_k(inputs)\n",
    "        v = self.dense_v(inputs)\n",
    "        print(q.shape)\n",
    "        print(k.shape)\n",
    "        print(v.shape)\n",
    "        q = self.split_heads(q, batch_size, self.dk)\n",
    "        k = self.split_heads(k, batch_size, self.dk)\n",
    "        v = self.split_heads(v, batch_size, self.dv)\n",
    "        print(q.shape)\n",
    "        print(k.shape)\n",
    "        print(v.shape)\n",
    "        \n",
    "        # \n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((self.num_heads, time_steps, time_steps)), -1, 0)\n",
    "        print(mask.shape)\n",
    "        \n",
    "        dk = tf.cast(self.dk, tf.float32)\n",
    "        print(dk.dtype,q.dtype,k.dtype)\n",
    "        \n",
    "        score = tf.nn.softmax(tf.matmul(q, k, transpose_b=True)/tf.math.sqrt(dk) + mask * -1e9)\n",
    "        \n",
    "        outputs = tf.matmul(score, v)\n",
    "        \n",
    "        outputs = tf.transpose(outputs, perm=[0, 2, 1, 3])\n",
    "        print(outputs.shape)\n",
    "        outputs = tf.squeeze(tf.concat(tf.split(outputs,self.num_heads,axis=2),axis=-1),axis=-2)\n",
    "        print(outputs.shape)\n",
    "#         outputs = tf.reshape(outputs, (batch_size, time_steps, -1))\n",
    "#         print(outputs.shape)\n",
    "        \n",
    "        outputs = self.dense1(outputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        \n",
    "        return outputs\n",
    "tensor = tf.constant([random.randint(0,20) for _ in range(23)],shape=[1,23,1],dtype=tf.float32)\n",
    "q_k_dim = 64\n",
    "v_dim = 64\n",
    "num_heads = 6\n",
    "kernel_size = 2 # 这个应该是kernel_size\n",
    "attention = Attention(q_k_dim,v_dim,num_heads,kernel_size)\n",
    "attention(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 23, 64), dtype=float32, numpy=\n",
       "array([[[ 0.09600929,  0.17820291,  0.2513949 , ...,  0.33010876,\n",
       "         -0.1308659 , -0.426734  ],\n",
       "        [ 0.13310671,  0.27456215,  0.36580214, ...,  0.4285784 ,\n",
       "         -0.2159098 , -0.69556177],\n",
       "        [ 0.2550217 ,  0.34249434,  0.37278575, ...,  0.41329986,\n",
       "         -0.18618973, -0.6584412 ],\n",
       "        ...,\n",
       "        [ 0.28866166,  0.5017709 ,  0.44298515, ...,  0.44206208,\n",
       "         -0.14011145, -0.77515763],\n",
       "        [ 0.47553673,  0.2951322 ,  0.30566636, ...,  0.36804664,\n",
       "         -0.03105145, -0.47216243],\n",
       "        [ 0.2822839 ,  0.47943288,  0.38585803, ...,  0.42223808,\n",
       "         -0.07439169, -0.72712535]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM 处理序列数据\n",
    "import random\n",
    "tensor = tf.constant([random.randint(0,20) for _ in range(23)],shape=[1,23,1],dtype=tf.float32)\n",
    "lstm = tf.keras.layers.LSTM(64, return_sequences=True, return_state=True)\n",
    "whole_seq_output, final_memory_state, final_carry_state = lstm(tensor)\n",
    "whole_seq_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
