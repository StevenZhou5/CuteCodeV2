{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20191002\n",
      "sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.1\n",
      "numpy 1.17.2\n",
      "pandas 0.25.1\n",
      "sklearn 0.21.3\n",
      "tensorflow 2.0.0-dev20191002\n",
      "tensorflow_core.keras 2.2.4-tf\n",
      "WARNING:tensorflow:From <ipython-input-1-0ef0fca1b7bc>:19: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.experimental.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl,np,pd,sklearn,tf,keras:\n",
    "    print(module.__name__,module.__version__)\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我们读取之前存储好的房价预测的csv文件，然后将其转换成tfrecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_15.csv', 'train_01.csv', 'train_00.csv', 'train_14.csv', 'test_08.csv', 'train_02.csv', 'train_16.csv', 'train_17.csv', 'train_03.csv', 'test_09.csv', 'train_07.csv', 'train_13.csv', 'train_12.csv', 'train_06.csv', 'train_10.csv', 'train_04.csv', 'train_05.csv', 'train_11.csv', 'valid_01.csv', 'valid_00.csv', 'valid_02.csv', 'valid_03.csv', 'valid_07.csv', 'valid_06.csv', 'valid_04.csv', 'valid_05.csv', 'valid_08.csv', 'valid_09.csv', 'test_02.csv', 'train_08.csv', 'train_09.csv', 'test_03.csv', 'test_01.csv', 'test_00.csv', 'test_04.csv', 'test_05.csv', 'test_07.csv', 'train_19.csv', 'train_18.csv', 'test_06.csv']\n",
      "['./generate_csv/train_00.csv',\n",
      " './generate_csv/train_01.csv',\n",
      " './generate_csv/train_02.csv',\n",
      " './generate_csv/train_03.csv',\n",
      " './generate_csv/train_04.csv',\n",
      " './generate_csv/train_05.csv',\n",
      " './generate_csv/train_06.csv',\n",
      " './generate_csv/train_07.csv',\n",
      " './generate_csv/train_08.csv',\n",
      " './generate_csv/train_09.csv',\n",
      " './generate_csv/train_10.csv',\n",
      " './generate_csv/train_11.csv',\n",
      " './generate_csv/train_12.csv',\n",
      " './generate_csv/train_13.csv',\n",
      " './generate_csv/train_14.csv',\n",
      " './generate_csv/train_15.csv',\n",
      " './generate_csv/train_16.csv',\n",
      " './generate_csv/train_17.csv',\n",
      " './generate_csv/train_18.csv',\n",
      " './generate_csv/train_19.csv']\n",
      "['./generate_csv/valid_00.csv',\n",
      " './generate_csv/valid_01.csv',\n",
      " './generate_csv/valid_02.csv',\n",
      " './generate_csv/valid_03.csv',\n",
      " './generate_csv/valid_04.csv',\n",
      " './generate_csv/valid_05.csv',\n",
      " './generate_csv/valid_06.csv',\n",
      " './generate_csv/valid_07.csv',\n",
      " './generate_csv/valid_08.csv',\n",
      " './generate_csv/valid_09.csv']\n",
      "['./generate_csv/test_00.csv',\n",
      " './generate_csv/test_01.csv',\n",
      " './generate_csv/test_02.csv',\n",
      " './generate_csv/test_03.csv',\n",
      " './generate_csv/test_04.csv',\n",
      " './generate_csv/test_05.csv',\n",
      " './generate_csv/test_06.csv',\n",
      " './generate_csv/test_07.csv',\n",
      " './generate_csv/test_08.csv',\n",
      " './generate_csv/test_09.csv']\n"
     ]
    }
   ],
   "source": [
    "source_dir = './generate_csv'\n",
    "print(os.listdir(source_dir)) # 这是40个csv文件\n",
    "\n",
    "def get_filenames_by_prefix(source_dir,prefix_name):\n",
    "    all_files = os.listdir(source_dir)\n",
    "    result = []\n",
    "    for file_name in all_files:\n",
    "        if file_name.startswith(prefix_name):\n",
    "            result.append(os.path.join(source_dir,file_name))\n",
    "    return sorted(result)\n",
    "\n",
    "import pprint\n",
    "train_filenames = get_filenames_by_prefix(source_dir,'train')\n",
    "pprint.pprint(train_filenames)\n",
    "\n",
    "valid_filenames = get_filenames_by_prefix(source_dir,'valid')\n",
    "pprint.pprint(valid_filenames)\n",
    "\n",
    "test_filenames = get_filenames_by_prefix(source_dir,'test')\n",
    "pprint.pprint(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# tf.stack 可以吧一个数组,np_array,tf_tensor都变成一个tf_tensor\n",
    "test = tf.constant([\n",
    "                 [1.,2.,3.],\n",
    "                 [4.,5.,6.]\n",
    "                ])\n",
    "print(test)\n",
    "print(tf.stack(test))\n",
    "print(tf.stack([\n",
    "                 [1.,2.,3.],\n",
    "                 [4.,5.,6.]\n",
    "                ]))\n",
    "print(tf.stack(np.array([\n",
    "                 [1.,2.,3.],\n",
    "                 [4.,5.,6.]\n",
    "                ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"stack_1:0\", shape=(1,), dtype=float32)\n",
      "Tensor(\"stack_1:0\", shape=(1,), dtype=float32)\n",
      "Tensor(\"stack_1:0\", shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def parse_csv_line(line_str, n_fields = 9):\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line_str,record_defaults=defs)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:])# 最后一个值时y, 注意这里一定要写成[-1:],不然返回的tensor是一个标量\n",
    "    print(y)\n",
    "    return x,y\n",
    "\n",
    "def csv_reader_dataset(filenames,n_readers=5,\n",
    "                       batch_size = 32,n_parse_threads=5,\n",
    "                       shuffle_buffer_size = 10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat() # 不传参数就是一个无限循环的generate\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line,\n",
    "                          num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 32\n",
    "train_set = csv_reader_dataset(train_filenames,\n",
    "                               batch_size = batch_size)\n",
    "valid_set = csv_reader_dataset(valid_filenames,\n",
    "                               batch_size = batch_size)\n",
    "test_set = csv_reader_dataset(test_filenames,\n",
    "                              batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将读取出来的csv文件数据存储到tfrecord文件中去\n",
    "\n",
    "# 定义一个序列化方法\n",
    "def serialize_example(x,y):\n",
    "    \"\"\"\n",
    "    Converts x,y to tf.train.Example and serialize\n",
    "    \"\"\"\n",
    "#     print(x)\n",
    "    input_features = tf.train.FloatList(value = x)\n",
    "#     print(y)\n",
    "    label = tf.train.FloatList(value = y)\n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            \"input_features\": tf.train.Feature(\n",
    "                float_list = input_features),\n",
    "            \"label\": tf.train.Feature(\n",
    "                float_list = label)\n",
    "        }\n",
    "    )\n",
    "    example = tf.train.Example(features = features)\n",
    "    return example.SerializeToString()\n",
    "\n",
    "# 将csv文件转换为tfrecords文件\n",
    "def csv_dataset_to_tfrecords(base_filename, # 存放tfrecord文件的文件夹路径\n",
    "                             dataset, # csv文件读取出来的数据生成的dataset\n",
    "                             n_shards, # “碎片” 要把生成的tfrecord文件分成多少个\n",
    "                             step_per_shard, # 要去多少个batch才能取到一个完整的epoch数据集\n",
    "                             compression_type = None # 压缩类型\n",
    "                            ):\n",
    "    options = tf.io.TFRecordOptions(\n",
    "        compression_type = compression_type )\n",
    "    all_filenames = []\n",
    "    for shard_id in range(n_shards):\n",
    "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(\n",
    "            base_filename, shard_id, n_shards)\n",
    "        with tf.io.TFRecordWriter(filename_fullpath,options) as writer:\n",
    "            for x_batch,y_batch in dataset.take(step_per_shard):\n",
    "                for x_example , y_example in zip(x_batch,y_batch):\n",
    "                    writer.write(\n",
    "                        serialize_example(x_example,y_example))\n",
    "        all_filenames.append(filename_fullpath)\n",
    "    return all_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成不压缩的tfrecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shards = 20\n",
    "train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "valid_steps_per_shard = 3880 // batch_size // n_shards\n",
    "test_steps_per_shard = 5170 // batch_size // n_shards\n",
    "\n",
    "output_dir = \"generate_tfrecords\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir,\"train\")\n",
    "valid_basename = os.path.join(output_dir,\"valid\")\n",
    "test_basename = os.path.join(output_dir,\"test\")\n",
    "\n",
    "train_tfrecord_filenames = csv_dataset_to_tfrecords(train_basename,\n",
    "                                                    train_set,\n",
    "                                                    n_shards,\n",
    "                                                    train_steps_per_shard,\n",
    "                                                    None)\n",
    "valid_tfrecord_filenames = csv_dataset_to_tfrecords(valid_basename,\n",
    "                                                    valid_set,\n",
    "                                                    n_shards,\n",
    "                                                    valid_steps_per_shard,\n",
    "                                                    None)\n",
    "test_tfrecord_filenames = csv_dataset_to_tfrecords(test_basename,\n",
    "                                                   test_set,\n",
    "                                                   n_shards,\n",
    "                                                   test_steps_per_shard,\n",
    "                                                   None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成压缩后的tfrecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shards = 20\n",
    "train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "valid_steps_per_shard = 3880 // batch_size // n_shards\n",
    "test_steps_per_shard = 5170 // batch_size // n_shards\n",
    "\n",
    "output_dir = \"generate_tfrecords_zip\" # 文件名称改一下用来区分\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir,\"train\")\n",
    "valid_basename = os.path.join(output_dir,\"valid\")\n",
    "test_basename = os.path.join(output_dir,\"test\")\n",
    "\n",
    "train_tfrecord_filenames = csv_dataset_to_tfrecords(train_basename,\n",
    "                                                    train_set,\n",
    "                                                    n_shards,\n",
    "                                                    train_steps_per_shard,\n",
    "                                                    compression_type = \"GZIP\") # 只用修改这一个地方\n",
    "valid_tfrecord_filenames = csv_dataset_to_tfrecords(valid_basename,\n",
    "                                                    valid_set,\n",
    "                                                    n_shards,\n",
    "                                                    valid_steps_per_shard,\n",
    "                                                    compression_type = \"GZIP\")\n",
    "test_tfrecord_filenames = csv_dataset_to_tfrecords(test_basename,\n",
    "                                                   test_set,\n",
    "                                                   n_shards,\n",
    "                                                   test_steps_per_shard,\n",
    "                                                   compression_type = \"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取tfrecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_tfrecords_zip/train_00000-of-00020',\n",
      " 'generate_tfrecords_zip/train_00001-of-00020',\n",
      " 'generate_tfrecords_zip/train_00002-of-00020',\n",
      " 'generate_tfrecords_zip/train_00003-of-00020',\n",
      " 'generate_tfrecords_zip/train_00004-of-00020',\n",
      " 'generate_tfrecords_zip/train_00005-of-00020',\n",
      " 'generate_tfrecords_zip/train_00006-of-00020',\n",
      " 'generate_tfrecords_zip/train_00007-of-00020',\n",
      " 'generate_tfrecords_zip/train_00008-of-00020',\n",
      " 'generate_tfrecords_zip/train_00009-of-00020',\n",
      " 'generate_tfrecords_zip/train_00010-of-00020',\n",
      " 'generate_tfrecords_zip/train_00011-of-00020',\n",
      " 'generate_tfrecords_zip/train_00012-of-00020',\n",
      " 'generate_tfrecords_zip/train_00013-of-00020',\n",
      " 'generate_tfrecords_zip/train_00014-of-00020',\n",
      " 'generate_tfrecords_zip/train_00015-of-00020',\n",
      " 'generate_tfrecords_zip/train_00016-of-00020',\n",
      " 'generate_tfrecords_zip/train_00017-of-00020',\n",
      " 'generate_tfrecords_zip/train_00018-of-00020',\n",
      " 'generate_tfrecords_zip/train_00019-of-00020']\n",
      "['generate_tfrecords_zip/valid_00000-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00001-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00002-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00003-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00004-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00005-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00006-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00007-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00008-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00009-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00010-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00011-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00012-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00013-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00014-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00015-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00016-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00017-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00018-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00019-of-00020']\n",
      "['generate_tfrecords_zip/test_00000-of-00020',\n",
      " 'generate_tfrecords_zip/test_00001-of-00020',\n",
      " 'generate_tfrecords_zip/test_00002-of-00020',\n",
      " 'generate_tfrecords_zip/test_00003-of-00020',\n",
      " 'generate_tfrecords_zip/test_00004-of-00020',\n",
      " 'generate_tfrecords_zip/test_00005-of-00020',\n",
      " 'generate_tfrecords_zip/test_00006-of-00020',\n",
      " 'generate_tfrecords_zip/test_00007-of-00020',\n",
      " 'generate_tfrecords_zip/test_00008-of-00020',\n",
      " 'generate_tfrecords_zip/test_00009-of-00020',\n",
      " 'generate_tfrecords_zip/test_00010-of-00020',\n",
      " 'generate_tfrecords_zip/test_00011-of-00020',\n",
      " 'generate_tfrecords_zip/test_00012-of-00020',\n",
      " 'generate_tfrecords_zip/test_00013-of-00020',\n",
      " 'generate_tfrecords_zip/test_00014-of-00020',\n",
      " 'generate_tfrecords_zip/test_00015-of-00020',\n",
      " 'generate_tfrecords_zip/test_00016-of-00020',\n",
      " 'generate_tfrecords_zip/test_00017-of-00020',\n",
      " 'generate_tfrecords_zip/test_00018-of-00020',\n",
      " 'generate_tfrecords_zip/test_00019-of-00020']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_tfrecord_filenames)\n",
    "pprint.pprint(valid_tfrecord_filenames)\n",
    "pprint.pprint(test_tfrecord_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 8), (None, 1)), types: (tf.float32, tf.float32)>\n",
      "tf.Tensor(\n",
      "[[ 0.09734604  0.75276285 -0.20218964 -0.19547    -0.40605137  0.00678553\n",
      "  -0.81371516  0.6566148 ]\n",
      " [ 0.63034356  1.8741661  -0.06713215 -0.12543367 -0.19737554 -0.02272263\n",
      "  -0.69240725  0.72652334]\n",
      " [ 2.5150437   1.0731637   0.5574401  -0.17273512 -0.6129126  -0.01909157\n",
      "  -0.5710993  -0.02749031]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.119  ]\n",
      " [2.419  ]\n",
      " [5.00001]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.0591781   1.3935647  -0.02633197 -0.1100676  -0.6138199  -0.09695935\n",
      "   0.3247131  -0.03747724]\n",
      " [-1.119975   -1.3298433   0.14190045  0.4658137  -0.10301778 -0.10744184\n",
      "  -0.7950524   1.5304717 ]\n",
      " [ 0.8015443   0.27216142 -0.11624393 -0.20231152 -0.5430516  -0.02103962\n",
      "  -0.5897621  -0.08241846]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.672]\n",
      " [0.66 ]\n",
      " [3.226]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 定义要读取的一条训练数据中各个数据的类型与长度，\n",
    "expected_features = {\n",
    "    \"input_features\": tf.io.FixedLenFeature([8],dtype=tf.float32),\n",
    "    \"label\": tf.io.FixedLenFeature([1],dtype=tf.float32)\n",
    "}\n",
    "\n",
    "def parse_example(serialized_example):\n",
    "    example = tf.io.parse_single_example(serialized_example,\n",
    "                                         expected_features)\n",
    "    return example[\"input_features\"],example[\"label\"]\n",
    "\n",
    "def tfrecord_reader_dataset(filenames,n_readers=5,\n",
    "                            batch_size=32,n_parse_threads=5,\n",
    "                            shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat() # 无限循环\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename:tf.data.TFRecordDataset(filename,compression_type=\"GZIP\"),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_example,\n",
    "                          num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "# 我们尝试去取数据\n",
    "tfrecords_train = tfrecord_reader_dataset(train_tfrecord_filenames,batch_size=3)\n",
    "\n",
    "print(tfrecords_train)\n",
    "for x_batch,y_batch in tfrecords_train.take(2):\n",
    "    print(x_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们取出正式训练的数据集\n",
    "batch_size = 32\n",
    "tfrecord_train_set = tfrecord_reader_dataset(train_tfrecord_filenames,\n",
    "                                             batch_size=batch_size)\n",
    "tfrecord_valid_set = tfrecord_reader_dataset(valid_tfrecord_filenames,\n",
    "                                             batch_size=batch_size)\n",
    "tfrecord_test_set = tfrecord_reader_dataset(test_tfrecord_filenames,\n",
    "                                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我们在模型中使用读取出来的tfrecord数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 348 steps, validate for 120 steps\n",
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 1.4736 - val_loss: 0.7977\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 0s 975us/step - loss: 0.5161 - val_loss: 0.5389\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 0s 998us/step - loss: 0.4389 - val_loss: 0.4635\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.4359\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.4393\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.4333\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 0.4484\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.4421\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.4333\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation='relu',\n",
    "                       input_shape=[8]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\",optimiezer='sgd')\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=5,min_delta=1e-2)\n",
    "]\n",
    "\n",
    "history = model.fit(tfrecord_train_set,\n",
    "                    validation_data = tfrecord_valid_set,\n",
    "                    steps_per_epoch = 11160//batch_size,\n",
    "                    validation_steps = 3870//batch_size,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 685us/step - loss: 0.4039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40388731487234186"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tfrecord_test_set,steps=5160//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
